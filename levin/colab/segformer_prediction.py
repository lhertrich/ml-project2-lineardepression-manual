# -*- coding: utf-8 -*-
"""segformer_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6z5SThKlvjM-RMII8INauRzBwmDxNHr
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/ML project 2/levin')

import os
import numpy as np
import torch.nn.functional as F
import torch
import matplotlib.image as mpimg
import re
import matplotlib.pyplot as plt
import shutil
from PIL import Image
from SegFormer.segformer_b3 import SegFormer
from dataset_submission import RoadSegmentationDataset
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation
from torch.utils.data import DataLoader

model_path = "/content/drive/My Drive/ML project 2/levin/trained_models/segformer_b3/DiceLoss()/segformer.pt"
base_path = "/content/drive/My Drive/ML project 2/levin/trained_models/segformer_b3/DiceLoss()"
model = SegFormer()

# Load the checkpoint
checkpoint = torch.load(model_path, map_location=model.device)

# Load the model state_dict
model.model.load_state_dict(checkpoint['model_state_dict'])

# Move model to device
model.model.to(model.device)

print("Model loaded successfully!")

def patch_to_label(patch, threshold=0.25):
    """
    Assign a label to a patch based on the average pixel intensity.
    :param patch: 16x16 patch of the predicted mask.
    :param threshold: Foreground threshold for binary classification.
    :return: Binary label (0 or 1).
    """
    df = np.mean(patch)
    if np.isnan(df):
        print(f"Encountered NaN in patch mean: {patch}")
    if patch.size == 0:
        print("Encountered empty patch!")
    if df > threshold:
        return 1
    else:
        return 0

def get_test_images(test_folder):
    """
    Recursively retrieve all test image file paths from the test folder.
    :param test_folder: Path to the folder containing test images in subfolders.
    :return: List of file paths to test images.
    """
    image_paths = []
    for root, _, files in os.walk(test_folder):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_paths.append(os.path.join(root, file))
    return image_paths

def mask_to_submission_strings(image_filename, threshold):
    """Reads a single image and outputs the strings that should go into the submission file"""
    img_number = int(re.search(r"prediction_(\d+)", image_filename).group(1))
    print(f"Filename: {image_filename}, number: {img_number}")
    im = mpimg.imread(image_filename)
    patch_size = 16
    for j in range(0, im.shape[1], patch_size):
        for i in range(0, im.shape[0], patch_size):
            patch = im[i:i + patch_size, j:j + patch_size]
            label = patch_to_label(patch, threshold)
            yield("{:03d}_{}_{},{}".format(img_number, j, i, label))

def masks_to_submission(submission_filename, *image_filenames, threshold):
    """Converts images into a submission file"""
    with open(submission_filename, 'w') as f:
        f.write('id,prediction\n')
        for fn in image_filenames[0:]:
            f.writelines('{}\n'.format(s) for s in mask_to_submission_strings(fn, threshold))

def resize_predictions(predictions, target_size=(608, 608)):
    """
    Resize a batch of predictions to the target size.
    :param predictions: Batch of predicted masks (numpy array or tensor of shape [batch_size, 1, H, W])
    :param target_size: Target size for resizing (e.g., (608, 608))
    :return: Resized predictions as a numpy array
    """
    if isinstance(predictions, np.ndarray):  # Convert numpy array to tensor if needed
        predictions = torch.from_numpy(predictions)

    # Ensure the tensor is in float32 for resizing
    predictions = predictions.to(dtype=torch.float32)

    # Resize predictions using bilinear interpolation
    resized_predictions = F.interpolate(predictions, size=target_size, mode="nearest")

    return resized_predictions.cpu().numpy()

def create_and_save_predictions(dataloader, model, device, save_path):
  prediction_dir = os.path.join(save_path, "prediction", "predictions")
  prediction_image_dir = os.path.join(save_path, "prediction", "images")

  # Remove existing directories and their contents
  if os.path.exists(prediction_dir):
    shutil.rmtree(prediction_dir)
  if os.path.exists(prediction_image_dir):
    shutil.rmtree(prediction_image_dir)

  # Recreate directories
  os.makedirs(prediction_dir, exist_ok=True)
  os.makedirs(prediction_image_dir, exist_ok=True)

  # Counter to keep track of the prediction files
  prediction_count = 1

  with torch.no_grad():
    for batch in dataloader:
      # Move images to device
      images = batch.to(device)

      # Get predictions
      predictions = model.predict(images)

      # Resize predictions (if necessary)
      predictions = resize_predictions(predictions)

      # Iterate over each prediction in the batch
      for pred in predictions:
        print("Saving prediction: ", prediction_count)
        #print(f"Pred unique values: {np.unique(pred)}")
        raw_pred_path = os.path.join(prediction_dir, f"prediction_{prediction_count}.png")
        binary_pred = (pred.squeeze() * 255).astype(np.uint8)
        binary_image = Image.fromarray(binary_pred)
        binary_image.save(raw_pred_path)

        # Save the black-and-white (scaled) prediction as a PNG
        bw_pred = (pred.squeeze() * 255).astype(np.uint8)  # Scale to [0, 255]
        bw_image = Image.fromarray(bw_pred)
        bw_image_path = os.path.join(prediction_image_dir, f"prediction_image_{prediction_count}.png")
        bw_image.save(bw_image_path)

        # Increment prediction counter
        prediction_count += 1

  print(f"Saved {prediction_count} predictions to {prediction_dir}")

test_images_folder = "/content/drive/My Drive/ML project 2/data/test_set_images"

test_image_paths = get_test_images(test_images_folder)
test_image_paths.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))

# Define transforms for images and masks
feature_extractor = SegformerImageProcessor(
    do_normalize=True,
    do_resize=True,
    size=512
)

dataset = RoadSegmentationDataset(test_images_folder, test_image_paths, feature_extractor)
dataloader = DataLoader(dataset, batch_size=8, num_workers=2)

create_and_save_predictions(dataloader, model, model.device, base_path)

image_filename = os.path.join(base_path, "prediction", "predictions", "prediction_1.png")
pred_im = mpimg.imread(image_filename)
pred_values = np.unique(pred_im)
print(pred_values)

submission_file = os.path.join(base_path, "submission_025.csv")
prediction_images_folder = os.path.join(base_path, "prediction", "predictions")

prediction_image_paths = [
    os.path.join(prediction_images_folder, filename)
    for filename in os.listdir(prediction_images_folder)
]

# Sort the paths based on the numeric part in the file name
prediction_image_paths.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))

masks_to_submission(submission_file, *prediction_image_paths, threshold=0.25)
print("Saved submission file")

model.model


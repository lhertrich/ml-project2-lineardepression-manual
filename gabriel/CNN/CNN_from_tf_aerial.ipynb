{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it correctly load the data - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images: True\n",
      "Test Images: True\n",
      "Training Images Files: ['satImage_001.png', 'satImage_002.png', 'satImage_003.png', 'satImage_004.png', 'satImage_005.png']\n",
      "Test Images Files: ['satImage_001.png', 'satImage_002.png', 'satImage_003.png', 'satImage_004.png', 'satImage_005.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_images_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\images\"\n",
    "test_images_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\groundtruth\"\n",
    "\n",
    "# Check if the directories exist\n",
    "print(\"Training Images:\", os.path.exists(train_images_dir))\n",
    "print(\"Test Images:\", os.path.exists(test_images_dir))\n",
    "\n",
    "# Check if there are files in the directories\n",
    "print(\"Training Images Files:\", os.listdir(train_images_dir)[:5])  # List first 5 files\n",
    "print(\"Test Images Files:\", os.listdir(test_images_dir)[:5])  # List first 5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st implementation # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\gabri\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_25212\\203297225.py\", line 3, in <cell line: 3>\n",
      "    import matplotlib.image as mpimg\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\", line 109, in <module>\n",
      "    from . import _api, _version, cbook, docstring, rcsetup\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, cbook, scale\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\scale.py\", line 23, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\ticker.py\", line 136, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"c:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\", line 46, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpimg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:109\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, docstring, rcsetup\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mplDeprecation  \u001b[38;5;66;03m# deprecated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py:56\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, scale\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\scale.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, docstring\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[0;32m     25\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[0;32m     26\u001b[0m     SymmetricalLogLocator, LogitLocator)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\ticker.py:136\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[0;32m    138\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    140\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    141\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    142\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\lib\\site-packages\\matplotlib\\transforms.py:46\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     47\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     50\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.image as mpimg\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.utils.data as data\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# NUM_CHANNELS = 3  # RGB images\n",
    "# PIXEL_DEPTH = 255\n",
    "# NUM_LABELS = 2\n",
    "# TRAINING_SIZE = 20\n",
    "# VALIDATION_SIZE = 5\n",
    "# SEED = 66478\n",
    "# BATCH_SIZE = 16\n",
    "# NUM_EPOCHS = 100\n",
    "# RESTORE_MODEL = False\n",
    "# RECORDING_STEP = 0\n",
    "# IMG_PATCH_SIZE = 16\n",
    "\n",
    "# # Random Seed Initialization for reproducibility\n",
    "# torch.manual_seed(SEED)\n",
    "\n",
    "# # Dataset class for loading images and labels\n",
    "# class SatelliteDataset(Dataset):\n",
    "#     def __init__(self, image_dir, label_dir, transform=None):\n",
    "#         self.image_dir = image_dir\n",
    "#         self.label_dir = label_dir\n",
    "#         self.transform = transform\n",
    "#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "#         self.label_files = [f for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_files)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image_file = self.image_files[idx]\n",
    "#         label_file = self.label_files[idx]\n",
    "\n",
    "#         image = mpimg.imread(os.path.join(self.image_dir, image_file))\n",
    "#         label = mpimg.imread(os.path.join(self.label_dir, label_file))\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         label = self.transform(label)\n",
    "\n",
    "#         return image, label\n",
    "\n",
    "# # Data transformation\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # Image cropping function to split into patches\n",
    "# def img_crop(im, w, h):\n",
    "#     patches = []\n",
    "#     imgwidth = im.shape[0]\n",
    "#     imgheight = im.shape[1]\n",
    "#     for i in range(0, imgheight, h):\n",
    "#         for j in range(0, imgwidth, w):\n",
    "#             im_patch = im[j:j+w, i:i+h, :] if len(im.shape) > 2 else im[j:j+w, i:i+h]\n",
    "#             patches.append(im_patch)\n",
    "#     return patches\n",
    "\n",
    "# # Convert label array to 1-hot encoding\n",
    "# def value_to_class(v):\n",
    "#     foreground_threshold = 0.25\n",
    "#     df = np.sum(v)\n",
    "#     return [0, 1] if df > foreground_threshold else [1, 0]\n",
    "\n",
    "# # CNN Model class definition\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(NUM_CHANNELS, 32, kernel_size=5, stride=1, padding=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "#         self.fc1 = nn.Linear(64 * (IMG_PATCH_SIZE // 4) * (IMG_PATCH_SIZE // 4), 512)\n",
    "#         self.fc2 = nn.Linear(512, NUM_LABELS)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = x.view(-1, 64 * (IMG_PATCH_SIZE // 4) * (IMG_PATCH_SIZE // 4))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Error rate calculation\n",
    "# def error_rate(predictions, labels):\n",
    "#     pred_labels = predictions.argmax(dim=1)\n",
    "#     true_labels = labels.argmax(dim=1)\n",
    "#     correct = (pred_labels == true_labels).float()\n",
    "#     return 100.0 - correct.sum() * 100.0 / len(pred_labels)\n",
    "\n",
    "# # Load data\n",
    "# train_data_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\images\"\n",
    "# train_labels_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\groundtruth\"\n",
    "# train_dataset = SatelliteDataset(train_data_dir, train_labels_dir, transform)\n",
    "# train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Initialize model, loss function, and optimizer\n",
    "# model = CNNModel()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Momentum(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# # Training loop\n",
    "# def train_model():\n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         running_error = 0.0\n",
    "\n",
    "#         for i, (inputs, labels) in enumerate(train_loader):\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             # Compute loss\n",
    "#             loss = loss_fn(outputs, labels)\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Optimize weights\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Compute error rate\n",
    "#             error = error_rate(outputs, labels)\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             running_error += error\n",
    "\n",
    "#             if (i + 1) % 10 == 0:  # Log every 10 batches\n",
    "#                 print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{i + 1}/{len(train_loader)}], \"\n",
    "#                       f\"Loss: {loss.item():.4f}, Error: {error:.4f}\")\n",
    "\n",
    "#         print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "#               f\"Error: {running_error / len(train_loader):.4f}\")\n",
    "\n",
    "# # Model Training\n",
    "# train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd implementation # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Set path and parameters\n",
    "\n",
    "# NUM_CHANNELS = 3  # RGB images\n",
    "# PIXEL_DEPTH = 255\n",
    "# NUM_LABELS = 2  # For binary classification\n",
    "# TRAINING_SIZE = 20\n",
    "# VALIDATION_SIZE = 5\n",
    "# SEED = 66478\n",
    "# BATCH_SIZE = 16\n",
    "# NUM_EPOCHS = 100\n",
    "# RESTORE_MODEL = False\n",
    "# RECORDING_STEP = 0\n",
    "# IMG_PATCH_SIZE = 16\n",
    "\n",
    "# Random Seed Initialization for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Dataset class for loading images and labels\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(\".png\")]\n",
    "        self.label_files = [f for f in os.listdir(label_dir) if f.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "\n",
    "\n",
    "### Doesn't work for some matplotlib incompatibility ###\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.image_files[idx]\n",
    "        label_file = self.label_files[idx]\n",
    "\n",
    "        image = mpimg.imread(os.path.join(self.image_dir, image_file))\n",
    "        label = mpimg.imread(os.path.join(self.label_dir, label_file))\n",
    "\n",
    "        # Ensure label is in integer format (class index)\n",
    "        label = np.array(label, dtype=np.uint8)\n",
    "        label = self.value_to_class(label)  # Convert to 1-hot encoded label\n",
    "        label = torch.tensor(label, dtype=torch.float32)  # Convert label to tensor\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    # Convert label array to 1-hot encoding\n",
    "    def value_to_class(self, v):\n",
    "        foreground_threshold = 0.25\n",
    "        df = np.sum(v)\n",
    "        return [0, 1] if df > foreground_threshold else [1, 0]\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "########################################## v1 CNN model class  #####################\n",
    "\n",
    "# # CNN Model class definition\n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(NUM_CHANNELS, 32, kernel_size=5, stride=1, padding=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "#         self.fc1 = nn.Linear(64 * (IMG_PATCH_SIZE // 4) * (IMG_PATCH_SIZE // 4), 512)\n",
    "#         self.fc2 = nn.Linear(512, NUM_LABELS)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "#         x = x.view(-1, 64 * (IMG_PATCH_SIZE // 4) * (IMG_PATCH_SIZE // 4))\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "#v2 CNN model \n",
    "    \n",
    "# class CNNModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNNModel, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(NUM_CHANNELS, 32, kernel_size=5, stride=1, padding=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Initialize fc1 after dynamically calculating the output size\n",
    "#         self._initialize_fc1()\n",
    "\n",
    "#         self.fc2 = nn.Linear(512, 1)  # 1 output unit for binary classification\n",
    "\n",
    "#     def _initialize_fc1(self):\n",
    "#         # Pass a dummy input through the conv layers to calculate size\n",
    "#         dummy_input = torch.zeros(1, NUM_CHANNELS, IMG_PATCH_SIZE, IMG_PATCH_SIZE)\n",
    "#         x = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         self.fc1 = nn.Linear(x.numel(), 512)  # Dynamically determine the input size for fc1\n",
    "\n",
    "#v3\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(NUM_CHANNELS, 16, kernel_size=5, stride=1, padding=2) \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = None  # To be initialized after forward pass\n",
    "        self.fc2 = nn.Linear(512, NUM_LABELS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Print the shape of x before flattening\n",
    "        print(f\"Shape before flattening: {x.shape}\")\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Initialize fc1 dynamically based on the flattened size\n",
    "        if self.fc1 is None:  # Initialize only once\n",
    "            self.fc1 = nn.Linear(x.size(1), 512)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = F.relu(self.conv1(x))\n",
    "    #     x = self.pool(x)\n",
    "    #     x = F.relu(self.conv2(x))\n",
    "    #     x = self.pool(x)\n",
    "    #     x = x.view(x.size(0), -1)  # Flatten\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "    #     return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Error rate calculation\n",
    "def error_rate(predictions, labels):\n",
    "    pred_labels = predictions.argmax(dim=1)\n",
    "    true_labels = labels.argmax(dim=1)\n",
    "    correct = (pred_labels == true_labels).float()\n",
    "    return 100.0 - correct.sum() * 100.0 / len(pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_error = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move inputs and labels to the device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute error rate\n",
    "            error = error_rate(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_error += error\n",
    "\n",
    "            if (i + 1) % 10 == 0:  # Log every 10 batches\n",
    "                print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{i + 1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}, Error: {error:.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {running_loss / len(train_loader):.4f}, \"\n",
    "              f\"Error: {running_error / len(train_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\images\"\n",
    "train_labels_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\groundtruth\"\n",
    "train_dataset = SatelliteDataset(train_data_dir, train_labels_dir, transform)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path and parameters\n",
    "\n",
    "NUM_CHANNELS = 3  # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2  # For binary classification\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5\n",
    "SEED = 66478\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 5\n",
    "RESTORE_MODEL = False\n",
    "RECORDING_STEP = 0\n",
    "IMG_PATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([4, 32, 100, 100])\n",
      "Epoch [1/5], Loss: 0.6287, Error: 0.0000\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([4, 32, 100, 100])\n",
      "Epoch [2/5], Loss: 0.3500, Error: 0.0000\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([4, 32, 100, 100])\n",
      "Epoch [3/5], Loss: 0.0339, Error: 0.0000\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([4, 32, 100, 100])\n",
      "Epoch [4/5], Loss: 0.0002, Error: 0.0000\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([16, 32, 100, 100])\n",
      "Shape before flattening: torch.Size([4, 32, 100, 100])\n",
      "Epoch [5/5], Loss: 0.0000, Error: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying on test_set ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get predicted class (road=1, background=0)\n",
    "        _, preds = torch.max(outputs, 1)  # Get class with the highest logit\n",
    "\n",
    "        # Process predictions and true labels for evaluation\n",
    "        all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU\n",
    "        all_labels.extend(labels.cpu().numpy())  # Same for true labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\images\"\n",
    "train_labels_dir = r\"C:\\Users\\gabri\\Desktop\\1 - Machine Learning\\Project 2\\ml-project2-lineardepression-manual\\data\\training\\groundtruth\"\n",
    "train_dataset = SatelliteDataset(train_data_dir, train_labels_dir, transform)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to make submissions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a trained model and the following variables are set:\n",
    "# model, test_loader, device, IMG_PATCH_SIZE = 16 (patch size)\n",
    "\n",
    "def split_image_into_patches(image, patch_size=16):\n",
    "    \"\"\"\n",
    "    Split an image into patches of size (patch_size x patch_size)\n",
    "    Returns a list of patches and the corresponding coordinates\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    coords = []\n",
    "    width, height = image.size\n",
    "    \n",
    "    for y in range(0, height, patch_size):\n",
    "        for x in range(0, width, patch_size):\n",
    "            patch = image.crop((x, y, x + patch_size, y + patch_size))\n",
    "            patches.append(patch)\n",
    "            coords.append((x, y))\n",
    "    \n",
    "    return patches, coords\n",
    "\n",
    "def get_predictions(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    with torch.no_grad():  # No need for gradients during inference\n",
    "        for folder in os.listdir(test_loader.dataset.root_dir):\n",
    "            folder_path = os.path.join(test_loader.dataset.root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                image_path = os.path.join(folder_path, 'image.png')  # Adjust the filename if needed\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # Split the image into patches\n",
    "                patches, coords = split_image_into_patches(image, patch_size=IMG_PATCH_SIZE)\n",
    "                \n",
    "                # Apply transformations (resize, convert to tensor, etc.)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((256, 256)),  # Resize to a fixed size or use the model's input size\n",
    "                    transforms.ToTensor(),\n",
    "                ])\n",
    "                \n",
    "                patch_predictions = []\n",
    "                \n",
    "                # Process each patch and get predictions\n",
    "                for patch, coord in zip(patches, coords):\n",
    "                    patch_tensor = transform(patch).unsqueeze(0).to(device)  # Add batch dimension\n",
    "                    output = model(patch_tensor)  # Forward pass\n",
    "                    pred = torch.argmax(output, dim=1).item()  # Get predicted label (0 or 1)\n",
    "                    \n",
    "                    # Generate the ID (e.g., '001_0_0' where 001 is the image id, and 0_0 are the coordinates)\n",
    "                    patch_id = f\"{folder}_{coord[0]}_{coord[1]}\"\n",
    "                    patch_predictions.append((patch_id, pred))\n",
    "                \n",
    "                # Add results for this image\n",
    "                all_results.extend(patch_predictions)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Function to generate the submission file\n",
    "def generate_submission(model, test_data_dir, device, output_file='submission.csv'):\n",
    "    # Prepare the test loader\n",
    "    test_dataset = TestDataset(root_dir=test_data_dir, transform=None)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # You can set batch_size to 1 for patch-wise predictions\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = get_predictions(model, test_loader, device)\n",
    "    \n",
    "    # Convert predictions into a DataFrame\n",
    "    df = pd.DataFrame(predictions, columns=['id', 'prediction'])\n",
    "    \n",
    "    # Save the predictions to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Submission file saved as {output_file}\")\n",
    "\n",
    "# Example of using the function:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generate_submission(model, test_data_dir=\"path_to_test_images\", device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Levin folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Construct the absolute path to the data folder\n",
    "# # image_dir = \n",
    "# # mask_dir = \n",
    "\n",
    "# # print(f\"Image directory: {image_dir}\")\n",
    "# # print(f\"Mask directory: {mask_dir}\")\n",
    "\n",
    "# # Check if the paths exist\n",
    "# if not os.path.exists(image_dir):\n",
    "#     raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "# if not os.path.exists(mask_dir):\n",
    "#     raise FileNotFoundError(f\"Mask directory not found: {mask_dir}\")\n",
    "\n",
    "# # Define transforms for images and masks\n",
    "# image_transform = T.Compose([\n",
    "#     T.Resize((512, 512)),  # Resize images to 512x512\n",
    "#     T.ToTensor(),          # Convert to PyTorch tensor\n",
    "#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "# ])\n",
    "\n",
    "# mask_transform = T.Compose([\n",
    "#     T.Resize((512, 512)),  # Resize masks to 512x512\n",
    "#     T.ToTensor()           # Convert to PyTorch tensor (binary mask stays as float)\n",
    "# ])\n",
    "\n",
    "# image_filenames = sorted(os.listdir(image_dir))\n",
    "# mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# # Split filenames into train and test sets\n",
    "# #train_images, test_images, train_masks, test_masks = train_test_split(\n",
    "# #    image_filenames, mask_filenames, test_size=0.2, random_state=42\n",
    "# #)\n",
    "\n",
    "# # Define datasets for train and test sets\n",
    "# train_dataset = RoadSegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     transform=image_transform,\n",
    "#     target_transform=mask_transform\n",
    "# )\n",
    "\n",
    "# # test_dataset = RoadSegmentationDataset(\n",
    "# #     image_dir=image_dir,\n",
    "# #     mask_dir=mask_dir,\n",
    "# #     transform=image_transform,\n",
    "# #     target_transform=mask_transform\n",
    "# # )\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# save_path = os.path.join(project_path, \"levin\", \"trained_models\", \"segformer_raw.pt\")\n",
    "# model = SegFormer()\n",
    "# model.train(train_loader, criterion, epochs=10, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Construct the absolute path to the data folder\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mproject_path\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m mask_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'project_path' is not defined"
     ]
    }
   ],
   "source": [
    "# # Construct the absolute path to the data folder\n",
    "# #image_dir = os.path.join(project_path, \"data\", \"training\", \"augmented\", \"images\")\n",
    "# #mask_dir = os.path.join(project_path, \"data\", \"training\", \"augmented\", \"masks\")\n",
    "\n",
    "# print(f\"Image directory: {image_dir}\")\n",
    "# print(f\"Mask directory: {mask_dir}\")\n",
    "\n",
    "# # Check if the paths exist\n",
    "# if not os.path.exists(image_dir):\n",
    "#     raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
    "# if not os.path.exists(mask_dir):\n",
    "#     raise FileNotFoundError(f\"Mask directory not found: {mask_dir}\")\n",
    "\n",
    "# # Define transforms for images and masks\n",
    "# image_transform = T.Compose([\n",
    "#     T.Resize((512, 512)),  # Resize images to 512x512\n",
    "#     T.ToTensor(),          # Convert to PyTorch tensor\n",
    "#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "# ])\n",
    "\n",
    "# mask_transform = T.Compose([\n",
    "#     T.Resize((512, 512)),  # Resize masks to 512x512\n",
    "#     T.ToTensor()           # Convert to PyTorch tensor (binary mask stays as float)\n",
    "# ])\n",
    "\n",
    "# image_filenames = sorted(os.listdir(image_dir))\n",
    "# mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "# # Split filenames into train and test sets\n",
    "# #train_images, test_images, train_masks, test_masks = train_test_split(\n",
    "# #    image_filenames, mask_filenames, test_size=0.2, random_state=42\n",
    "# #)\n",
    "\n",
    "# # Define datasets for train and test sets\n",
    "# train_dataset = RoadSegmentationDataset(\n",
    "#     image_dir=image_dir,\n",
    "#     mask_dir=mask_dir,\n",
    "#     transform=image_transform,\n",
    "#     target_transform=mask_transform\n",
    "# )\n",
    "\n",
    "# # test_dataset = RoadSegmentationDataset(\n",
    "# #     image_dir=image_dir,\n",
    "# #     mask_dir=mask_dir,\n",
    "# #     transform=image_transform,\n",
    "# #     target_transform=mask_transform\n",
    "# # )\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
